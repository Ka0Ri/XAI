{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import neptune.new as neptune\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"kaori/XAI\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyZjZiMDA2YS02MDM3LTQxZjQtOTE4YS1jODZkMTJjNGJlMDYifQ==\",\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"lr\": 1e-2,\n",
    "    \"bs\": 128,\n",
    "    \"input_sz\": 32 * 32 * 3,\n",
    "    \"n_classes\": 10,\n",
    "}\n",
    "run[\"parameters\"] = params\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "trainset = datasets.CIFAR10(\"./data\", transform=transform, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=params[\"bs\"], shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_dim, n_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_sz, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input.view(-1, 32 * 32 * 3)\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "model = BaseModel(params[\"input_sz\"], params[\"input_sz\"], params[\"n_classes\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "for i, (x, y) in enumerate(trainloader, 0):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model.forward(x)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    loss = criterion(outputs, y)\n",
    "    acc = (torch.sum(preds == y.data)) / len(x)\n",
    "\n",
    "    run[\"train/batch/loss\"].append(loss)\n",
    "    run[\"train/batch/acc\"].append(acc)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "for X, y in trainloader:\n",
    "    pred = model(X)\n",
    "    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "run[\"valid/acc\"] = correct / len(trainloader.dataset)\n",
    "\n",
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7eefedad5f758f73641af163b7940175a79660ec24780dc6aa8e5a24ff91ea50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
